{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical Questions\n",
        "1.What is a Decision Tree, and how does it work?\n",
        "-> A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by recursively splitting the dataset into subsets based on feature values, creating a tree-like structure. Each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome or class label.\n",
        "\n",
        "2. What are impurity measures in Decision Trees?\n",
        "-> Impurity measures quantify the degree of disorder or uncertainty in a dataset. Common impurity measures include Gini Impurity and Entropy. Gini Impurity calculates the probability of misclassifying a randomly chosen element, while Entropy measures the unpredictability of information content. These measures help determine the best feature to split the data at each node.\n",
        "\n",
        "What is the mathematical formula for Gini Impurity?\n",
        "-> The Gini Impurity formula is given by:\n",
        "G\n",
        "i\n",
        "n\n",
        "i\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "C\n",
        "p\n",
        "i\n",
        "2\n",
        "Gini=1−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "  where\n",
        "p\n",
        "i\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of class\n",
        "i\n",
        "i in the dataset, and\n",
        "C\n",
        "C is the total number of classes. A lower Gini value indicates a purer node, while a higher value indicates more impurity.\n",
        "\n",
        "4. What is the mathematical formula for Entropy?\n",
        "-> The Entropy formula is defined as:\n",
        "E\n",
        "n\n",
        "t\n",
        "r\n",
        "o\n",
        "p\n",
        "y\n",
        "=\n",
        "−\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "C\n",
        "p\n",
        "i\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "p\n",
        "i\n",
        ")\n",
        "Entropy=−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " ) where\n",
        "p\n",
        "i\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of class\n",
        "i\n",
        "i in the dataset, and\n",
        "C\n",
        "C is the total number of classes. Entropy measures the uncertainty in the dataset; higher values indicate more disorder.\n",
        "\n",
        "5. What is Information Gain, and how is it used in Decision Trees?\n",
        "-> Information Gain measures the reduction in entropy or impurity after a dataset is split on a feature. It is calculated as:\n",
        "I\n",
        "G\n",
        "=\n",
        "E\n",
        "n\n",
        "t\n",
        "r\n",
        "o\n",
        "p\n",
        "y\n",
        "(\n",
        "p\n",
        "a\n",
        "r\n",
        "e\n",
        "n\n",
        "t\n",
        ")\n",
        "−\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "n\n",
        "∣\n",
        "D\n",
        "i\n",
        "∣\n",
        "∣\n",
        "D\n",
        "∣\n",
        "E\n",
        "n\n",
        "t\n",
        "r\n",
        "o\n",
        "p\n",
        "y\n",
        "(\n",
        "D\n",
        "i\n",
        ")\n",
        "IG=Entropy(parent)−∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        "  \n",
        "∣D∣\n",
        "∣D\n",
        "i\n",
        "​\n",
        " ∣\n",
        "​\n",
        " Entropy(D\n",
        "i\n",
        "​\n",
        " ) where\n",
        "D\n",
        "i\n",
        "D\n",
        "i\n",
        "​\n",
        "  are the subsets created by the split. Higher Information Gain indicates a better feature for splitting.\n",
        "\n",
        "  6.What is the difference between Gini Impurity and Entropy?\n",
        "  ->  Gini Impurity and Entropy are both measures of impurity used in Decision Trees. Gini focuses on the probability of misclassification, while Entropy measures the amount of information. Gini is computationally simpler and faster, while Entropy can provide more informative splits. However, both often yield similar results in practice.\n",
        "\n",
        "7. What is the mathematical explanation behind Decision Trees?\n",
        "-> Decision Trees use recursive partitioning to create a model based on feature values. The algorithm selects features that maximize Information Gain or minimize impurity (Gini or Entropy) at each node. The process continues until a stopping criterion is met, such as reaching a maximum depth or minimum samples per leaf.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees?\n",
        " ->Pre-Pruning, or early stopping, is a technique used to prevent overfitting in Decision Trees. It involves halting the growth of the tree before it reaches its full depth based on certain criteria, such as a minimum number of samples required to split a node or a maximum tree depth. This helps maintain model generalization.\n",
        "\n",
        "9. What is Post-Pruning in Decision Trees?\n",
        "-> Post-Pruning is a technique applied after the Decision Tree has been fully grown. It involves removing nodes that provide little predictive power, based on validation data. This process helps reduce overfitting by simplifying the model while retaining its accuracy. Common methods include cost complexity pruning and reduced error pruning.\n",
        "\n",
        "10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "-> Pre-Pruning occurs during the tree-building process, stopping growth based on specific criteria to prevent overfitting. In contrast, Post-Pruning occurs after the tree is fully grown, where nodes are removed to simplify the model. Pre-Pruning can be more efficient, while Post-Pruning often results in a more accurate model.\n",
        "\n",
        "11. What is a Decision Tree Regressor?\n",
        "-> A Decision Tree Regressor is a variant of Decision Trees used for regression tasks. Instead of predicting class labels, it predicts continuous values. The algorithm splits the data based on feature values to minimize the mean squared error (MSE) in the target variable. The final prediction is the average value of the target in the leaf node.\n",
        "\n",
        "12. What are the advantages and disadvantages of Decision Trees?\n",
        "-> Advantages of Decision Trees include their simplicity, interpretability, and ability to handle both numerical and categorical data. They require little data preprocessing and can model complex relationships. Disadvantages include susceptibility to overfitting, sensitivity to noisy data, and instability, as small changes in data can lead to different tree structures.\n",
        "\n",
        "13. How does a Decision Tree handle missing values?\n",
        "-> Decision Trees can handle missing values by using surrogate splits, which are alternative features that can be used to make decisions when the primary feature is missing. During training, the algorithm can assign instances with missing values to the most common class or average value in the corresponding node, preserving information.\n",
        "\n",
        "14. How does a Decision Tree handle categorical features?\n",
        "-> Decision Trees can handle categorical features by creating binary splits for each category. The algorithm evaluates each category's impact on the target variable and selects the best split based on impurity measures (Gini or Entropy). This allows the tree to effectively partition the data based on categorical attributes.\n",
        "\n",
        "15. What are some real-world applications of Decision Trees?\n",
        "-> Decision Trees are widely used in various fields, including finance for credit scoring, healthcare for disease diagnosis, marketing for customer segmentation, and manufacturing for quality control. Their interpretability makes them valuable for decision-making processes, allowing stakeholders to understand the reasoning behind predictions and classifications."
      ],
      "metadata": {
        "id": "YH7Vw1h00U1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "TIgstETZ2uw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "JTzints12xSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "# Train Decision Tree Classifier with Gini Impurity\n",
        "dt_classifier_gini = DecisionTreeClassifier(criterion='gini')\n",
        "dt_classifier_gini.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = dt_classifier_gini.feature_importances_\n",
        "print(\"Feature importances:\", importances)"
      ],
      "metadata": {
        "id": "t-WjCCp-3BLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n",
        "# Train Decision Tree Classifier with Entropy\n",
        "dt_classifier_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "dt_classifier_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_entropy = dt_classifier_entropy.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "print(f\"Model accuracy with Entropy: {accuracy_entropy:.2f}\")"
      ],
      "metadata": {
        "id": "WQTXnfoc3Mwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_regressor = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred_regressor)\n",
        "print(f\"Mean Squared Error of Decision Tree Regressor: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "Fvc5asgk3Q0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier_viz = DecisionTreeClassifier()\n",
        "dt_classifier_viz.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree\n",
        "dot_data = export_graphviz(dt_classifier_viz, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\")  # Save the tree visualization\n",
        "graph.view()  # Open the visualization"
      ],
      "metadata": {
        "id": "4q-JYdl73YsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n",
        "# Train fully grown Decision Tree Classifier\n",
        "dt_classifier_full = DecisionTreeClassifier()\n",
        "dt_classifier_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_classifier_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train Decision Tree Classifier with max depth of 3\n",
        "dt_classifier_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "dt_classifier_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = dt_classifier_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy of fully grown tree: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy of tree with max depth 3: {accuracy_depth3:.2f}\")"
      ],
      "metadata": {
        "id": "VUOTybcL3dbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n",
        "# Train default Decision Tree Classifier\n",
        "dt_classifier_default = DecisionTreeClassifier()\n",
        "dt_classifier_default.fit(X_train, y_train)\n",
        "y_pred_default = dt_classifier_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train Decision Tree Classifier with min_samples_split=5\n",
        "dt_classifier_split5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "dt_classifier_split5.fit(X_train, y_train)\n",
        "y_pred_split5 = dt_classifier_split5.predict(X_test)\n",
        "accuracy_split5 = accuracy_score(y_test, y_pred_split5)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy of default tree: {accuracy_default:.2f}\")\n",
        "print(f\"Accuracy of tree with min_samples_split=5: {accuracy_split5:.2f}\")"
      ],
      "metadata": {
        "id": "RXr4Q61F3hgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Decision Tree Classifier on unscaled data\n",
        "dt_classifier_unscaled = DecisionTreeClassifier()\n",
        "dt_classifier_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = dt_classifier_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Train Decision Tree Classifier on scaled data\n",
        "dt_classifier_scaled = DecisionTreeClassifier()\n",
        "dt_classifier_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = dt_classifier_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy on unscaled data: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy on scaled data: {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "h3jNbbVZ3mUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Train One-vs-Rest Decision Tree Classifier\n",
        "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of One-vs-Rest Decision Tree Classifier: {accuracy_ovr:.2f}\")"
      ],
      "metadata": {
        "id": "M55fRFzT3tU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier_importance = DecisionTreeClassifier()\n",
        "dt_classifier_importance.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importance scores\n",
        "importances = dt_classifier_importance.feature_importances_\n",
        "print(\"Feature importance scores:\", importances)"
      ],
      "metadata": {
        "id": "0jUjTbKV32He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.\n",
        "# Train unrestricted Decision Tree Regressor\n",
        "dt_regressor_full = DecisionTreeRegressor()\n",
        "dt_regressor_full.fit(X_train, y_train)\n",
        "y_pred_full_regressor = dt_regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full_regressor)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth=5\n",
        "dt_regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "dt_regressor_depth5.fit(X_train, y_train)\n",
        "y_pred_depth5 = dt_regressor_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "# Compare MSE\n",
        "print(f\"MSE of unrestricted tree: {mse_full:.2f}\")\n",
        "print(f\"MSE of tree with max depth 5: {mse_depth5:.2f}\")"
      ],
      "metadata": {
        "id": "DiBr3KWb35tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier_ccp = DecisionTreeClassifier(ccp_alpha=0.01)  # Set CCP parameter\n",
        "dt_classifier_ccp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ccp = dt_classifier_ccp.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_ccp = accuracy_score(y_test, y_pred_ccp)\n",
        "print(f\"Accuracy after Cost Complexity Pruning: {accuracy_ccp:.2f}\")"
      ],
      "metadata": {
        "id": "9W_wYHUn3-lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier_eval = DecisionTreeClassifier()\n",
        "dt_classifier_eval.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_eval = dt_classifier_eval.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred_eval)\n",
        "recall = recall_score(y_test, y_pred_eval)\n",
        "f1 = f1_score(y_test, y_pred_eval)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "if0NIBrk4FIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_classifier_cm = DecisionTreeClassifier()\n",
        "dt_classifier_cm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_cm = dt_classifier_cm.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_cm)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u-ojR9FT4LNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier_grid = DecisionTreeClassifier()\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(dt_classifier_grid, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "T_X1-PH64XS_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}